"""
MVTec AD Dataset Preprocessing Script
MVTec Anomaly Detection ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã®å‰å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: wallplugs
- Normal train: 293æš
- Normal validation: 33æš  
- Anomalous: 90æš
- å…ƒã‚µã‚¤ã‚º: 2448 x 2048
- ç›®æ¨™ã‚µã‚¤ã‚º: 1024 x 1024
"""

import os
import sys
from pathlib import Path
from PIL import Image
import numpy as np
from tqdm import tqdm
import json
import logging
from datetime import datetime

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¨­å®š
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

class MVTecPreprocessor:
    """MVTec AD ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‰å‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, dataset_name="wallplugs", target_size=(1024, 1024)):
        self.dataset_name = dataset_name
        self.target_size = target_size
        self.source_dir = Path(f"data/images/{dataset_name}/TrainVald")
        self.output_dir = Path(f"data/processed/{dataset_name}")
        
        # ãƒ­ã‚°è¨­å®š
        self.logger = logging.getLogger(f"mvtec_preprocessor_{dataset_name}")
        self.logger.setLevel(logging.INFO)
        
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            "processed_count": 0,
            "original_sizes": [],
            "processing_errors": [],
            "dataset_info": {}
        }
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.output_dir.mkdir(parents=True, exist_ok=True)
        (self.output_dir / "train" / "normal").mkdir(parents=True, exist_ok=True)
        (self.output_dir / "train" / "anomalous").mkdir(parents=True, exist_ok=True)
        (self.output_dir / "validation" / "normal").mkdir(parents=True, exist_ok=True)
        (self.output_dir / "validation" / "anomalous").mkdir(parents=True, exist_ok=True)
        
    def get_image_info(self, image_path):
        """ç”»åƒã®åŸºæœ¬æƒ…å ±ã‚’å–å¾—"""
        try:
            with Image.open(image_path) as img:
                return {
                    "size": img.size,  # (width, height)
                    "mode": img.mode,
                    "format": img.format
                }
        except Exception as e:
            self.logger.error(f"ç”»åƒæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼ {image_path}: {e}")
            return None
    
    def resize_image(self, image_path, output_path):
        """ç”»åƒã‚’ãƒªã‚µã‚¤ã‚ºã—ã¦ä¿å­˜"""
        try:
            with Image.open(image_path) as img:
                # å…ƒã‚µã‚¤ã‚ºã‚’è¨˜éŒ²
                original_size = img.size
                self.stats["original_sizes"].append(original_size)
                
                # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’ä¿æŒã—ã¦ãƒªã‚µã‚¤ã‚º
                img_resized = img.resize(self.target_size, Image.Resampling.LANCZOS)
                
                # RGBå¤‰æ›ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
                if img_resized.mode != 'RGB':
                    img_resized = img_resized.convert('RGB')
                
                # ä¿å­˜
                img_resized.save(output_path, 'PNG', optimize=True)
                
                return True, original_size
                
        except Exception as e:
            error_msg = f"ç”»åƒå‡¦ç†ã‚¨ãƒ©ãƒ¼ {image_path}: {e}"
            self.logger.error(error_msg)
            self.stats["processing_errors"].append(error_msg)
            return False, None
    
    def process_directory(self, source_subdir, output_subdir, category="normal"):
        """æŒ‡å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å…¨ç”»åƒã‚’å‡¦ç†"""
        source_path = self.source_dir / source_subdir
        output_path = self.output_dir / output_subdir
        
        if not source_path.exists():
            self.logger.warning(f"ã‚½ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {source_path}")
            return 0
        
        # PNG ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
        image_files = list(source_path.glob("*.png"))
        processed_count = 0
        
        print(f"\nğŸ“ å‡¦ç†ä¸­: {source_subdir} â†’ {output_subdir}")
        print(f"   ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(image_files)}æš")
        
        for image_file in tqdm(image_files, desc=f"å‡¦ç†ä¸­ {category}"):
            output_file = output_path / image_file.name
            success, original_size = self.resize_image(image_file, output_file)
            
            if success:
                processed_count += 1
                self.stats["processed_count"] += 1
            
        return processed_count
    
    def analyze_dataset(self):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆ†æ"""
        print(f"\nğŸ” {self.dataset_name} ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†æé–‹å§‹")
        
        # ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã§æƒ…å ±ç¢ºèª
        sample_paths = [
            self.source_dir / "normal" / "train",
            self.source_dir / "anomalous"
        ]
        
        for sample_dir in sample_paths:
            if sample_dir.exists():
                sample_files = list(sample_dir.glob("*.png"))
                if sample_files:
                    sample_info = self.get_image_info(sample_files[0])
                    if sample_info:
                        print(f"   ğŸ“· ã‚µãƒ³ãƒ—ãƒ«ç”»åƒ: {sample_files[0].name}")
                        print(f"      ã‚µã‚¤ã‚º: {sample_info['size']} ({sample_info['size'][0]}Ã—{sample_info['size'][1]})")
                        print(f"      ãƒ¢ãƒ¼ãƒ‰: {sample_info['mode']}")
                        print(f"      å½¢å¼: {sample_info['format']}")
                        break
    
    def process_mvtec_dataset(self):
        """MVTec ADãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å®Œå…¨å‡¦ç†"""
        print(f"ğŸš€ MVTec AD {self.dataset_name} ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‰å‡¦ç†é–‹å§‹")
        print(f"   ã‚½ãƒ¼ã‚¹: {self.source_dir}")
        print(f"   å‡ºåŠ›: {self.output_dir}")
        print(f"   ç›®æ¨™ã‚µã‚¤ã‚º: {self.target_size[0]}Ã—{self.target_size[1]}")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†æ
        self.analyze_dataset()
        
        start_time = datetime.now()
        
        # 1. Normal Training Data (293æš)
        train_normal_count = self.process_directory(
            "normal/train", "train/normal", "normal_train"
        )
        
        # 2. Normal Validation Data (33æš)
        val_normal_count = self.process_directory(
            "normal/vald", "validation/normal", "normal_val"
        )
        
        # 3. Anomalous Data (90æš) - è¨“ç·´ç”¨ã«åˆ†å‰²
        anomalous_files = list((self.source_dir / "anomalous").glob("*.png"))
        
        # ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã‚’7:3ã§åˆ†å‰²ï¼ˆtrain:validationï¼‰
        np.random.seed(42)  # å†ç¾æ€§ã®ãŸã‚
        np.random.shuffle(anomalous_files)
        
        split_point = int(len(anomalous_files) * 0.7)
        train_anomalous = anomalous_files[:split_point]
        val_anomalous = anomalous_files[split_point:]
        
        # ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ - Training
        train_anomalous_count = 0
        print(f"\nğŸ“ å‡¦ç†ä¸­: anomalous â†’ train/anomalous")
        print(f"   ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(train_anomalous)}æš")
        
        for image_file in tqdm(train_anomalous, desc="å‡¦ç†ä¸­ anomalous_train"):
            output_file = self.output_dir / "train" / "anomalous" / image_file.name
            success, _ = self.resize_image(image_file, output_file)
            if success:
                train_anomalous_count += 1
                self.stats["processed_count"] += 1
        
        # ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ - Validation
        val_anomalous_count = 0
        print(f"\nğŸ“ å‡¦ç†ä¸­: anomalous â†’ validation/anomalous")
        print(f"   ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(val_anomalous)}æš")
        
        for image_file in tqdm(val_anomalous, desc="å‡¦ç†ä¸­ anomalous_val"):
            output_file = self.output_dir / "validation" / "anomalous" / image_file.name
            success, _ = self.resize_image(image_file, output_file)
            if success:
                val_anomalous_count += 1
                self.stats["processed_count"] += 1
        
        # å‡¦ç†æ™‚é–“è¨ˆç®—
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        # çµ±è¨ˆæƒ…å ±æ›´æ–°
        self.stats["dataset_info"] = {
            "dataset_name": self.dataset_name,
            "original_total": train_normal_count + val_normal_count + len(anomalous_files),
            "processed_total": self.stats["processed_count"],
            "train_normal": train_normal_count,
            "train_anomalous": train_anomalous_count,
            "val_normal": val_normal_count,
            "val_anomalous": val_anomalous_count,
            "target_size": self.target_size,
            "processing_time": processing_time
        }
        
        # çµæœå ±å‘Š
        self.print_summary()
        
        # çµ±è¨ˆæƒ…å ±ä¿å­˜
        self.save_statistics()
        
        return self.stats
    
    def print_summary(self):
        """å‡¦ç†çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        info = self.stats["dataset_info"]
        
        print("\n" + "="*60)
        print(f"ğŸ‰ MVTec AD {info['dataset_name']} å‰å‡¦ç†å®Œäº†")
        print("="*60)
        print(f"ğŸ“Š å‡¦ç†çµæœ:")
        print(f"   ç·å‡¦ç†æ•°: {info['processed_total']}æš")
        print(f"   è¨“ç·´ç”¨æ­£å¸¸: {info['train_normal']}æš")
        print(f"   è¨“ç·´ç”¨ç•°å¸¸: {info['train_anomalous']}æš")
        print(f"   æ¤œè¨¼ç”¨æ­£å¸¸: {info['val_normal']}æš")
        print(f"   æ¤œè¨¼ç”¨ç•°å¸¸: {info['val_anomalous']}æš")
        print(f"   ç›®æ¨™ã‚µã‚¤ã‚º: {info['target_size'][0]}Ã—{info['target_size'][1]}")
        print(f"   å‡¦ç†æ™‚é–“: {info['processing_time']:.1f}ç§’")
        
        if self.stats["processing_errors"]:
            print(f"\nâš ï¸  ã‚¨ãƒ©ãƒ¼: {len(self.stats['processing_errors'])}ä»¶")
        
        # å…ƒã‚µã‚¤ã‚ºçµ±è¨ˆ
        if self.stats["original_sizes"]:
            unique_sizes = list(set(self.stats["original_sizes"]))
            print(f"ğŸ“ å…ƒç”»åƒã‚µã‚¤ã‚º: {unique_sizes}")
        
        print(f"\nğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
        print("="*60)
    
    def save_statistics(self):
        """çµ±è¨ˆæƒ…å ±ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        stats_file = self.output_dir / "preprocessing_stats.json"
        
        # serializableã«å¤‰æ›
        serializable_stats = {
            "dataset_info": self.stats["dataset_info"],
            "processed_count": self.stats["processed_count"],
            "original_sizes": [list(size) for size in set(self.stats["original_sizes"])],
            "processing_errors": self.stats["processing_errors"],
            "timestamp": datetime.now().isoformat()
        }
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_stats, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ çµ±è¨ˆæƒ…å ±ä¿å­˜: {stats_file}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸš€ MVTec AD ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‰å‡¦ç†ãƒ„ãƒ¼ãƒ«")
    print("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: wallplugs")
    print("ç›®æ¨™ã‚µã‚¤ã‚º: 1024Ã—1024")
    
    # å‰å‡¦ç†å®Ÿè¡Œ
    preprocessor = MVTecPreprocessor(
        dataset_name="wallplugs",
        target_size=(1024, 1024)
    )
    
    stats = preprocessor.process_mvtec_dataset()
    
    # è¿½åŠ æƒ…å ±
    print("\nğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("1. data/processed/wallplugs/ ã§å‰å‡¦ç†æ¸ˆã¿ç”»åƒã‚’ç¢ºèª")
    print("2. MAD-FH ã‚·ã‚¹ãƒ†ãƒ ã§ã®å­¦ç¿’ãƒ»åˆ†æã«ä½¿ç”¨")
    print("3. ä»–ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆsheet_metal, wallnuts, fruit_jellyï¼‰ã‚‚åŒæ§˜ã«å‡¦ç†")

if __name__ == "__main__":
    main()