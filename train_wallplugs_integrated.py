""" MVTec AD Wallplugs çµ±åˆå­¦ç¿’ãƒãƒãƒ¼ã‚¸ãƒ£ ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«ï¼ˆMiniCPMï¼‰ã¨LoRAèª¬æ˜ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®çµ±åˆå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ  """ import sys import os from pathlib import Path import json import yaml import torch from datetime import datetime import subprocess import argparse # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¨­å®š project_root = Path(__file__).parent sys.path.insert(0, str(project_root)) sys.path.insert(0, str(project_root / "src")) class WallplugsTrainingManager: """Wallplugså­¦ç¿’ç®¡ç†ã‚¯ãƒ©ã‚¹""" def __init__(self): self.project_root = Path(__file__).parent self.models_dir = self.project_root / "models" self.models_dir.mkdir(exist_ok=True) # å­¦ç¿’çµæœè¨˜éŒ² self.training_results = { 'session_id': datetime.now().strftime('%Y%m%d_%H%M%S'), 'start_time': datetime.now().isoformat(), 'anomaly_detection': {}, 'lora_explanation': {}, 'integration_test': {}, 'summary': {} } print("MVTec AD Wallplugs çµ±åˆå­¦ç¿’ãƒãƒãƒ¼ã‚¸ãƒ£") print("="*60) def check_prerequisites(self): """å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯""" print("å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯ä¸­...") issues = [] # å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ç¢ºèª data_dir = Path("data/processed/wallplugs") if not data_dir.exists(): issues.append("å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (data/processed/wallplugs)") else: # ãƒ‡ãƒ¼ã‚¿å†…å®¹ç¢ºèª train_normal = data_dir / "train" / "normal" train_anomalous = data_dir / "train" / "anomalous" val_normal = data_dir / "validation" / "normal" val_anomalous = data_dir / "validation" / "anomalous" if not all([train_normal.exists(), train_anomalous.exists(), val_normal.exists(), val_anomalous.exists()]): issues.append("ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ãŒä¸å®Œå…¨ã§ã™") else: train_normal_count = len(list(train_normal.glob("*.png"))) train_anomalous_count = len(list(train_anomalous.glob("*.png"))) val_normal_count = len(list(val_normal.glob("*.png"))) val_anomalous_count = len(list(val_anomalous.glob("*.png"))) print(f" [OK] å­¦ç¿’ç”¨æ­£å¸¸ãƒ‡ãƒ¼ã‚¿: {train_normal_count}æš") print(f" [OK] å­¦ç¿’ç”¨ç•°å¸¸ãƒ‡ãƒ¼ã‚¿: {train_anomalous_count}æš") print(f" [OK] æ¤œè¨¼ç”¨æ­£å¸¸ãƒ‡ãƒ¼ã‚¿: {val_normal_count}æš") print(f" [OK] æ¤œè¨¼ç”¨ç•°å¸¸ãƒ‡ãƒ¼ã‚¿: {val_anomalous_count}æš") # GPUç¢ºèª if torch.cuda.is_available(): print(f" [OK] GPUåˆ©ç”¨å¯èƒ½: {torch.cuda.get_device_name()}") gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9 print(f" ãƒ¡ãƒ¢ãƒª: {gpu_memory:.1f}GB") if gpu_memory < 8.0: issues.append("GPU ãƒ¡ãƒ¢ãƒªãŒ8GBæœªæº€ã§ã™ã€‚å­¦ç¿’ã«æ™‚é–“ãŒã‹ã‹ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚") else: issues.append("GPU ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚CPU ãƒ¢ãƒ¼ãƒ‰ã§ã¯å­¦ç¿’ã«é•·æ™‚é–“ã‹ã‹ã‚Šã¾ã™ã€‚") # å¿…è¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç¢ºèª required_packages = ['transformers', 'peft', 'torch', 'PIL', 'sklearn'] for package in required_packages: try: __import__(package) except ImportError: issues.append(f"å¿…è¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {package}") if issues: print("\n[WARNING] å•é¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ:") for issue in issues: print(f" - {issue}") return False else: print(" [OK] ã™ã¹ã¦ã®å‰ææ¡ä»¶ãŒæº€ãŸã•ã‚Œã¦ã„ã¾ã™") return True def create_training_config(self): """çµ±åˆå­¦ç¿’è¨­å®šä½œæˆ""" config = { 'session_info': { 'session_id': self.training_results['session_id'], 'dataset': 'MVTec AD Wallplugs', 'description': 'MiniCPMç•°å¸¸æ¤œçŸ¥ + LoRAèª¬æ˜ç”Ÿæˆã®çµ±åˆå­¦ç¿’' }, 'minicpm_anomaly': { 'model': { 'latent_dim': 512, 'use_minicpm': True, 'minicpm_weight': 0.3, 'anomaly_threshold': 0.1 }, 'training': { 'batch_size': 2, # GPU ãƒ¡ãƒ¢ãƒªã«å¿œã˜ã¦èª¿æ•´ 'epochs': 20, 'learning_rate': 1e-4, 'weight_decay': 1e-5, 'patience': 8 } }, 'lora_explanation': { 'model': { 'name': 'Salesforce/blip2-opt-2.7b' }, 'lora': { 'r': 16, 'alpha': 32, 'dropout': 0.1, 'target_modules': ['q_proj', 'v_proj'] }, 'training': { 'epochs': 8, 'batch_size': 1, # LoRAã®ãŸã‚å°ã•ãè¨­å®š 'learning_rate': 5e-5, 'weight_decay': 0.01, 'warmup_steps': 50 } } } # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ config_path = self.models_dir / f"training_config_{self.training_results['session_id']}.yaml" with open(config_path, 'w', encoding='utf-8') as f: yaml.dump(config, f, default_flow_style=False, allow_unicode=True) print(f"[CONFIG] çµ±åˆå­¦ç¿’è¨­å®šä¿å­˜: {config_path}") return config, config_path def run_anomaly_detection_training(self, config): """ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Ÿè¡Œ""" print("\n" + "="*60) print(" Phase 1: MiniCPMç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«å­¦ç¿’") print("="*60) try: # MiniCPMå­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ start_time = datetime.now() # Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ result = subprocess.run([ sys.executable, "train_minicpm_wallplugs.py" ], capture_output=True, text=True, cwd=self.project_root) end_time = datetime.now() training_time = (end_time - start_time).total_seconds() if result.returncode == 0: print("[OK] ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†") # çµæœè¨˜éŒ² self.training_results['anomaly_detection'] = { 'status': 'success', 'training_time': training_time, 'output': result.stdout[-1000:], # æœ€å¾Œã®1000æ–‡å­— 'model_path': 'models/minicpm_autoencoder_wallplugs_best.pth' } return True else: print("[FAILED] ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å¤±æ•—") print("ã‚¨ãƒ©ãƒ¼å‡ºåŠ›:") print(result.stderr) self.training_results['anomaly_detection'] = { 'status': 'failed', 'error': result.stderr, 'output': result.stdout } return False except Exception as e: print(f"[FAILED] ç•°å¸¸æ¤œçŸ¥å­¦ç¿’å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}") self.training_results['anomaly_detection'] = { 'status': 'error', 'error': str(e) } return False def run_lora_training(self, config): """LoRAèª¬æ˜ç”Ÿæˆãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Ÿè¡Œ""" print("\n" + "="*60) print(" Phase 2: LoRAèª¬æ˜ç”Ÿæˆãƒ¢ãƒ‡ãƒ«å­¦ç¿’") print("="*60) try: start_time = datetime.now() # LoRAå­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ result = subprocess.run([ sys.executable, "train_lora_wallplugs.py" ], capture_output=True, text=True, cwd=self.project_root) end_time = datetime.now() training_time = (end_time - start_time).total_seconds() if result.returncode == 0: print("[OK] LoRAèª¬æ˜ç”Ÿæˆãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†") self.training_results['lora_explanation'] = { 'status': 'success', 'training_time': training_time, 'output': result.stdout[-1000:], 'model_path': 'models/lora_wallplugs/final_model' } return True else: print("[FAILED] LoRAèª¬æ˜ç”Ÿæˆãƒ¢ãƒ‡ãƒ«å­¦ç¿’å¤±æ•—") print("ã‚¨ãƒ©ãƒ¼å‡ºåŠ›:") print(result.stderr) self.training_results['lora_explanation'] = { 'status': 'failed', 'error': result.stderr, 'output': result.stdout } return False except Exception as e: print(f"[FAILED] LoRAå­¦ç¿’å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}") self.training_results['lora_explanation'] = { 'status': 'error', 'error': str(e) } return False def run_integration_test(self): """çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ""" print("\n" + "="*60) print(" Phase 3: çµ±åˆãƒ†ã‚¹ãƒˆ") print("="*60) try: # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§FODDãƒ†ã‚¹ãƒˆå®Ÿè¡Œ start_time = datetime.now() result = subprocess.run([ sys.executable, "test_wallplugs_fodd.py" ], capture_output=True, text=True, cwd=self.project_root) end_time = datetime.now() test_time = (end_time - start_time).total_seconds() if result.returncode == 0: print("[OK] çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†") self.training_results['integration_test'] = { 'status': 'success', 'test_time': test_time, 'output': result.stdout[-1000:] } return True else: print("[WARNING] çµ±åˆãƒ†ã‚¹ãƒˆã§å•é¡Œç™ºç”Ÿï¼ˆç¶™ç¶šå¯èƒ½ï¼‰") self.training_results['integration_test'] = { 'status': 'warning', 'test_time': test_time, 'error': result.stderr, 'output': result.stdout } return True # çµ±åˆãƒ†ã‚¹ãƒˆã¯å¤±æ•—ã—ã¦ã‚‚OK except Exception as e: print(f"[FAILED] çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}") self.training_results['integration_test'] = { 'status': 'error', 'error': str(e) } return True # çµ±åˆãƒ†ã‚¹ãƒˆã¯å¤±æ•—ã—ã¦ã‚‚OK def generate_final_report(self): """æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ""" print("\n" + "="*60) print("[REPORT] æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ") print("="*60) # å­¦ç¿’çµæœã‚µãƒãƒªãƒ¼ anomaly_success = self.training_results['anomaly_detection'].get('status') == 'success' lora_success = self.training_results['lora_explanation'].get('status') == 'success' integration_ok = self.training_results['integration_test'].get('status') in ['success', 'warning'] overall_success = anomaly_success and lora_success # ã‚µãƒãƒªãƒ¼ä½œæˆ self.training_results['summary'] = { 'overall_status': 'success' if overall_success else 'partial' if (anomaly_success or lora_success) else 'failed', 'anomaly_detection_success': anomaly_success, 'lora_explanation_success': lora_success, 'integration_test_ok': integration_ok, 'total_training_time': ( self.training_results['anomaly_detection'].get('training_time', 0) + self.training_results['lora_explanation'].get('training_time', 0) ), 'end_time': datetime.now().isoformat() } # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ report_path = self.models_dir / f"training_report_{self.training_results['session_id']}.json" with open(report_path, 'w', encoding='utf-8') as f: json.dump(self.training_results, f, indent=2, ensure_ascii=False, default=str) # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ› print(f"ğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}") print(f"\n å­¦ç¿’ã‚»ãƒƒã‚·ãƒ§ãƒ³å®Œäº†") print(f" ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {self.training_results['session_id']}") print(f" å…¨ä½“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {self.training_results['summary']['overall_status'].upper()}") print(f" ç•°å¸¸æ¤œçŸ¥å­¦ç¿’: {'[OK]' if anomaly_success else '[FAILED]'}") print(f" LoRAèª¬æ˜ç”Ÿæˆ: {'[OK]' if lora_success else '[FAILED]'}") print(f" çµ±åˆãƒ†ã‚¹ãƒˆ: {'[OK]' if integration_ok else '[WARNING]'}") print(f" ç·å­¦ç¿’æ™‚é–“: {self.training_results['summary']['total_training_time']:.1f}ç§’") if overall_success: print(f"\nğŸ‰ ã™ã¹ã¦ã®å­¦ç¿’ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼") print(f"[TIP] æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:") print(f" 1. Streamlit UIã§ã®å‹•ä½œç¢ºèª") print(f" 2. ä»–ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆsheet_metal, wallnuts, fruit_jellyï¼‰ã§ã®å­¦ç¿’") print(f" 3. æœ¬æ ¼é‹ç”¨ç’°å¢ƒã§ã®å±•é–‹") else: print(f"\n[WARNING] ä¸€éƒ¨ã®å­¦ç¿’ã§å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ") print(f" è©³ç´°ã¯ {report_path} ã‚’ç¢ºèªã—ã¦ãã ã•ã„") return report_path def run_full_training(self, skip_anomaly=False, skip_lora=False): """å®Œå…¨çµ±åˆå­¦ç¿’å®Ÿè¡Œ""" print(f"é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}") # å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯ if not self.check_prerequisites(): print("[FAILED] å‰ææ¡ä»¶ãŒæº€ãŸã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å­¦ç¿’ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚") return False # å­¦ç¿’è¨­å®šä½œæˆ config, config_path = self.create_training_config() success = True # Phase 1: ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ if not skip_anomaly: if not self.run_anomaly_detection_training(config): success = False else: print("ğŸ”„ ç•°å¸¸æ¤œçŸ¥å­¦ç¿’ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ") # Phase 2: LoRAèª¬æ˜ç”Ÿæˆå­¦ç¿’ if not skip_lora: if not self.run_lora_training(config): success = False else: print("ğŸ”„ LoRAå­¦ç¿’ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ") # Phase 3: çµ±åˆãƒ†ã‚¹ãƒˆ self.run_integration_test() # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ report_path = self.generate_final_report() return success, report_path def main(): """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ""" parser = argparse.ArgumentParser(description='MVTec AD Wallplugs çµ±åˆå­¦ç¿’ãƒãƒãƒ¼ã‚¸ãƒ£') parser.add_argument('--skip-anomaly', action='store_true', help='ç•°å¸¸æ¤œçŸ¥å­¦ç¿’ã‚’ã‚¹ã‚­ãƒƒãƒ—') parser.add_argument('--skip-lora', action='store_true', help='LoRAå­¦ç¿’ã‚’ã‚¹ã‚­ãƒƒãƒ—') args = parser.parse_args() try: manager = WallplugsTrainingManager() success, report_path = manager.run_full_training( skip_anomaly=args.skip_anomaly, skip_lora=args.skip_lora ) if success: print(f"\n[OK] çµ±åˆå­¦ç¿’ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼") exit(0) else: print(f"\n[FAILED] çµ±åˆå­¦ç¿’ã§å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚") exit(1) except KeyboardInterrupt: print(f"\nâ¹ï¸ å­¦ç¿’ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚") exit(2) except Exception as e: print(f"\nğŸ’¥ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}") import traceback traceback.print_exc() exit(3) if __name__ == "__main__": main()