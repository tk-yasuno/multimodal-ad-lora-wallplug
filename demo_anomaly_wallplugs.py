"""
MVTec AD Wallplugs è»½é‡ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ‡ãƒ¢
MiniCPMã‚’ä½¿ã‚ãªã„è»½é‡ç‰ˆç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ†ã‚¹ãƒˆ
"""

import os
import sys
from pathlib import Path
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import numpy as np
from tqdm import tqdm
import json
from datetime import datetime

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¨­å®š
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

class SimpleWallplugsDataset(Dataset):
    """è»½é‡Wallplugsãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    
    def __init__(self, data_dir, split='train', max_samples=50, image_size=256):
        self.data_dir = Path(data_dir)
        self.split = split
        self.max_samples = max_samples
        self.image_size = image_size
        
        # ã‚µãƒ³ãƒ—ãƒ«åé›†
        self.samples = []
        self.collect_samples()
        
        print(f"{split} dataset: {len(self.samples)} samples")
    
    def collect_samples(self):
        """ã‚µãƒ³ãƒ—ãƒ«åé›†"""
        # æ­£å¸¸ç”»åƒ
        normal_dir = self.data_dir / self.split / "normal"
        if normal_dir.exists():
            normal_files = list(normal_dir.glob("*.png"))[:self.max_samples//2]
            for img_path in normal_files:
                self.samples.append({
                    'image_path': str(img_path),
                    'label': 0  # 0: normal
                })
        
        # ç•°å¸¸ç”»åƒ
        anomalous_dir = self.data_dir / self.split / "anomalous"
        if anomalous_dir.exists():
            anomalous_files = list(anomalous_dir.glob("*.png"))[:self.max_samples//2]
            for img_path in anomalous_files:
                self.samples.append({
                    'image_path': str(img_path),
                    'label': 1  # 1: anomalous
                })
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # ç”»åƒèª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†
        image = Image.open(sample['image_path']).convert('RGB')
        image = image.resize((self.image_size, self.image_size))
        
        # numpy -> tensor
        image_array = np.array(image) / 255.0
        image_tensor = torch.from_numpy(image_array).permute(2, 0, 1).float()
        
        return {
            'image': image_tensor,
            'label': torch.tensor(sample['label'], dtype=torch.float32),
            'path': sample['image_path']
        }

class LightweightAutoencoder(nn.Module):
    """è»½é‡ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼"""
    
    def __init__(self, input_channels=3, latent_dim=128, image_size=256):
        super().__init__()
        
        self.input_channels = input_channels
        self.latent_dim = latent_dim
        self.image_size = image_size
        
        # Encoder
        self.encoder = nn.Sequential(
            # 256x256 -> 128x128
            nn.Conv2d(input_channels, 32, 4, stride=2, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            
            # 128x128 -> 64x64
            nn.Conv2d(32, 64, 4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            
            # 64x64 -> 32x32
            nn.Conv2d(64, 128, 4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            
            # 32x32 -> 16x16
            nn.Conv2d(128, 256, 4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            
            # Global Average Pooling
            nn.AdaptiveAvgPool2d((4, 4)),
            nn.Flatten(),
            nn.Linear(256 * 4 * 4, latent_dim)
        )
        
        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 256 * 4 * 4),
            nn.ReLU(inplace=True),
            nn.Unflatten(1, (256, 4, 4)),
            
            # 4x4 -> 8x8
            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            
            # 8x8 -> 16x16
            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            
            # 16x16 -> 32x32
            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            
            # 32x32 -> 64x64
            nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(inplace=True),
            
            # 64x64 -> 128x128
            nn.ConvTranspose2d(16, 8, 4, stride=2, padding=1),
            nn.BatchNorm2d(8),
            nn.ReLU(inplace=True),
            
            # 128x128 -> 256x256
            nn.ConvTranspose2d(8, input_channels, 4, stride=2, padding=1),
            nn.Sigmoid()
        )
        
        # ç•°å¸¸æ¤œçŸ¥ãƒ˜ãƒƒãƒ‰
        self.anomaly_head = nn.Sequential(
            nn.Linear(latent_dim, latent_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(latent_dim // 2, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        latent = self.encoder(x)
        
        # ãƒ‡ã‚³ãƒ¼ãƒ‰
        reconstructed = self.decoder(latent)
        
        # ç•°å¸¸ã‚¹ã‚³ã‚¢
        anomaly_score = self.anomaly_head(latent)
        
        return {
            'reconstructed': reconstructed,
            'latent': latent,
            'anomaly_score': anomaly_score.squeeze()
        }

class WallplugsLightweightTrainer:
    """è»½é‡ç•°å¸¸æ¤œçŸ¥å­¦ç¿’ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"Device: {self.device}")
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.output_dir = Path("models/lightweight_anomaly")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # å­¦ç¿’å±¥æ­´
        self.training_history = {
            'loss': [],
            'reconstruction_loss': [],
            'anomaly_loss': []
        }
    
    def setup_data(self):
        """ãƒ‡ãƒ¼ã‚¿æº–å‚™"""
        data_dir = Path("data/processed/wallplugs")
        
        if not data_dir.exists():
            print("[ERROR] å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        self.train_dataset = SimpleWallplugsDataset(
            data_dir, split='train', max_samples=60, image_size=256
        )
        self.val_dataset = SimpleWallplugsDataset(
            data_dir, split='validation', max_samples=20, image_size=256
        )
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        self.train_loader = DataLoader(
            self.train_dataset, 
            batch_size=4,
            shuffle=True,
            num_workers=2
        )
        
        self.val_loader = DataLoader(
            self.val_dataset,
            batch_size=4,
            shuffle=False,
            num_workers=2
        )
        
        return True
    
    def setup_model(self):
        """ãƒ¢ãƒ‡ãƒ«æº–å‚™"""
        self.model = LightweightAutoencoder(
            input_channels=3,
            latent_dim=128,
            image_size=256
        ).to(self.device)
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=1e-3,
            weight_decay=1e-5
        )
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=10
        )
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°è¡¨ç¤º
        total_params = sum(p.numel() for p in self.model.parameters())
        print(f"Total parameters: {total_params:,}")
    
    def compute_loss(self, output, target, labels):
        """æå¤±è¨ˆç®—"""
        reconstructed = output['reconstructed']
        anomaly_scores = output['anomaly_score']
        
        # å†æ§‹æˆæå¤±
        reconstruction_loss = nn.MSELoss()(reconstructed, target)
        
        # ç•°å¸¸æ¤œçŸ¥æå¤±
        anomaly_loss = nn.BCELoss()(anomaly_scores, labels)
        
        # ç·æå¤±
        total_loss = reconstruction_loss + 0.5 * anomaly_loss
        
        return total_loss, reconstruction_loss, anomaly_loss
    
    def train_epoch(self, epoch):
        """1ã‚¨ãƒãƒƒã‚¯å­¦ç¿’"""
        self.model.train()
        
        total_loss = 0.0
        total_recon_loss = 0.0
        total_anomaly_loss = 0.0
        num_batches = 0
        
        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}')
        
        for batch in pbar:
            images = batch['image'].to(self.device)
            labels = batch['label'].to(self.device)
            
            self.optimizer.zero_grad()
            
            # é †ä¼æ’­
            output = self.model(images)
            
            # æå¤±è¨ˆç®—
            loss, recon_loss, anomaly_loss = self.compute_loss(output, images, labels)
            
            # é€†ä¼æ’­
            loss.backward()
            self.optimizer.step()
            
            # æå¤±è“„ç©
            total_loss += loss.item()
            total_recon_loss += recon_loss.item()
            total_anomaly_loss += anomaly_loss.item()
            num_batches += 1
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼æ›´æ–°
            pbar.set_postfix({
                'Loss': f"{loss.item():.4f}",
                'Recon': f"{recon_loss.item():.4f}",
                'Anomaly': f"{anomaly_loss.item():.4f}"
            })
        
        # ã‚¨ãƒãƒƒã‚¯å¹³å‡
        avg_loss = total_loss / num_batches
        avg_recon_loss = total_recon_loss / num_batches
        avg_anomaly_loss = total_anomaly_loss / num_batches
        
        return avg_loss, avg_recon_loss, avg_anomaly_loss
    
    def validate(self):
        """æ¤œè¨¼"""
        self.model.eval()
        
        total_loss = 0.0
        predictions = []
        ground_truth = []
        
        with torch.no_grad():
            for batch in self.val_loader:
                images = batch['image'].to(self.device)
                labels = batch['label'].to(self.device)
                
                # é †ä¼æ’­
                output = self.model(images)
                
                # æå¤±
                loss, _, _ = self.compute_loss(output, images, labels)
                total_loss += loss.item()
                
                # äºˆæ¸¬åé›†
                anomaly_scores = output['anomaly_score'].cpu().numpy()
                labels_np = labels.cpu().numpy()
                
                predictions.extend(anomaly_scores)
                ground_truth.extend(labels_np)
        
        avg_loss = total_loss / len(self.val_loader)
        
        # AUCè¨ˆç®—
        try:
            from sklearn.metrics import roc_auc_score
            auc_score = roc_auc_score(ground_truth, predictions)
        except:
            auc_score = 0.0
        
        return avg_loss, auc_score
    
    def train(self, epochs=10):
        """å­¦ç¿’å®Ÿè¡Œ"""
        print(f"\n[TRAIN] Lightweight Anomaly Detection Training")
        print("="*50)
        
        best_auc = 0.0
        
        for epoch in range(epochs):
            print(f"\nEpoch {epoch+1}/{epochs}")
            print("-" * 30)
            
            # å­¦ç¿’
            train_loss, train_recon, train_anomaly = self.train_epoch(epoch)
            
            # æ¤œè¨¼
            val_loss, val_auc = self.validate()
            
            # å±¥æ­´è¨˜éŒ²
            self.training_history['loss'].append(train_loss)
            self.training_history['reconstruction_loss'].append(train_recon)
            self.training_history['anomaly_loss'].append(train_anomaly)
            
            # ãƒ­ã‚°å‡ºåŠ›
            print(f"Train Loss: {train_loss:.4f} (Recon: {train_recon:.4f}, Anomaly: {train_anomaly:.4f})")
            print(f"Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}")
            
            # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜
            if val_auc > best_auc:
                best_auc = val_auc
                self.save_model(epoch, val_auc)
                print(f"ğŸ‰ New best model! AUC: {val_auc:.4f}")
            
            # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼æ›´æ–°
            self.scheduler.step()
        
        print(f"\n[SUCCESS] Training completed!")
        print(f"Best validation AUC: {best_auc:.4f}")
        
        return best_auc
    
    def save_model(self, epoch, auc):
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        model_path = self.output_dir / "lightweight_autoencoder_best.pth"
        
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'epoch': epoch,
            'auc': auc,
            'training_history': self.training_history
        }, model_path)
        
        # å­¦ç¿’å±¥æ­´ä¿å­˜
        history_path = self.output_dir / "training_history.json"
        with open(history_path, 'w') as f:
            json.dump(self.training_history, f, indent=2)

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("[DEMO] MVTec AD Wallplugs Lightweight Anomaly Detection")
    print("="*60)
    
    try:
        trainer = WallplugsLightweightTrainer()
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ¢ãƒ‡ãƒ«æº–å‚™
        if not trainer.setup_data():
            return False
        
        trainer.setup_model()
        
        # å­¦ç¿’å®Ÿè¡Œ
        best_auc = trainer.train(epochs=8)
        
        print(f"\n[SUCCESS] Training completed successfully!")
        print(f"Best AUC: {best_auc:.4f}")
        print(f"Model saved: models/lightweight_anomaly/")
        
        return True
        
    except Exception as e:
        print(f"\n[FAILED] Training failed: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    main()