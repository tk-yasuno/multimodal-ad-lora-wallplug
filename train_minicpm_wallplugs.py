"""
MVTec AD Wallplugs Ã— MiniCPM ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
MiniCPMçµ±åˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’å®Ÿè¡Œ
"""

import os
import sys
from pathlib import Path
import torch
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from PIL import Image
import numpy as np
import yaml
from tqdm import tqdm
import logging
from datetime import datetime
import json
import matplotlib.pyplot as plt

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¨­å®š
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

from src.models.minicpm_autoencoder import MiniCPMAnomalyDetector, MiniCPMHybridAutoencoder
from src.utils.logger import setup_logger

class WallplugsDataset(Dataset):
    """MVTec AD Wallplugs ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    
    def __init__(self, data_dir, split='train', transform=None):
        self.data_dir = Path(data_dir)
        self.split = split
        self.transform = transform
        
        # ç”»åƒãƒ‘ã‚¹ã¨ãƒ©ãƒ™ãƒ«ã‚’å–å¾—
        self.samples = []
        
        # æ­£å¸¸ç”»åƒ
        normal_dir = self.data_dir / split / "normal"
        if normal_dir.exists():
            for img_path in normal_dir.glob("*.png"):
                self.samples.append((str(img_path), 0))  # 0: normal
        
        # ç•°å¸¸ç”»åƒ
        anomalous_dir = self.data_dir / split / "anomalous"
        if anomalous_dir.exists():
            for img_path in anomalous_dir.glob("*.png"):
                self.samples.append((str(img_path), 1))  # 1: anomalous
        
        print(f"{split} dataset: {len(self.samples)} samples")
        print(f"  Normal: {sum(1 for _, label in self.samples if label == 0)}")
        print(f"  Anomalous: {sum(1 for _, label in self.samples if label == 1)}")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        img_path, label = self.samples[idx]
        
        # ç”»åƒèª­ã¿è¾¼ã¿
        image = Image.open(img_path).convert('RGB')
        
        # å‰å‡¦ç†
        if self.transform:
            image = self.transform(image)
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¤‰æ›
            image = np.array(image) / 255.0
            image = torch.from_numpy(image).permute(2, 0, 1).float()
        
        return {
            'images': image,
            'anomaly_labels': torch.tensor(label, dtype=torch.float32),
            'image_path': img_path
        }

class WallplugsTrainer:
    """Wallplugsãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå­¦ç¿’ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ãƒ­ã‚°è¨­å®š
        self.logger = setup_logger('wallplugs_trainer')
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        self.setup_data()
        
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        self.setup_model()
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼è¨­å®š
        self.setup_optimizer()
        
        # è¨˜éŒ²ç”¨
        self.train_history = {
            'total_loss': [],
            'reconstruction_loss': [],
            'anomaly_loss': [],
            'validation_score': []
        }
    
    def _safe_item(self, value):
        """å®‰å…¨ãªå€¤å–å¾—ï¼ˆtensorã§ã‚‚numberã§ã‚‚å¯¾å¿œï¼‰"""
        if isinstance(value, torch.Tensor):
            if value.numel() == 1:  # ã‚¹ã‚«ãƒ©ãƒ¼tensor
                return value.item()
            else:
                return float(value.mean().item())  # è¤‡æ•°è¦ç´ ã®å ´åˆã¯å¹³å‡
        else:
            return float(value)  # æ—¢ã«æ•°å€¤ã®å ´åˆ
    
    def setup_data(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼æº–å‚™"""
        data_dir = Path("data/processed/wallplugs")
        
        if not data_dir.exists():
            raise ValueError("å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚preprocess_mvtec.pyã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        self.train_dataset = WallplugsDataset(data_dir, split='train')
        self.val_dataset = WallplugsDataset(data_dir, split='validation')
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ
        self.train_loader = DataLoader(
            self.train_dataset,
            batch_size=self.config['training']['batch_size'],
            shuffle=True,
            num_workers=2,
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        self.val_loader = DataLoader(
            self.val_dataset,
            batch_size=self.config['training']['batch_size'],
            shuffle=False,
            num_workers=2,
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        self.logger.info(f"Data setup complete:")
        self.logger.info(f"  Train samples: {len(self.train_dataset)}")
        self.logger.info(f"  Val samples: {len(self.val_dataset)}")
        self.logger.info(f"  Batch size: {self.config['training']['batch_size']}")
    
    def setup_model(self):
        """ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–"""
        model_config = {
            'input_channels': 3,
            'latent_dim': self.config['model']['latent_dim'],
            'input_size': (1024, 1024),
            'use_minicpm': self.config['model'].get('use_minicpm', True),
            'minicpm_weight': self.config['model'].get('minicpm_weight', 0.3),
            'anomaly_threshold': self.config['model']['anomaly_threshold']
        }
        
        self.detector = MiniCPMAnomalyDetector(model_config, device=self.device)
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤º
        total_params = sum(p.numel() for p in self.detector.model.parameters())
        trainable_params = sum(p.numel() for p in self.detector.model.parameters() if p.requires_grad)
        
        self.logger.info(f"Model setup complete:")
        self.logger.info(f"  Total parameters: {total_params:,}")
        self.logger.info(f"  Trainable parameters: {trainable_params:,}")
        self.logger.info(f"  Device: {self.device}")
        self.logger.info(f"  MiniCPM enabled: {model_config['use_minicpm']}")
    
    def setup_optimizer(self):
        """ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼è¨­å®š"""
        self.optimizer = optim.AdamW(
            self.detector.model.parameters(),
            lr=self.config['training']['learning_rate'],
            weight_decay=self.config['training']['weight_decay']
        )
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=self.config['training']['epochs']
        )
        
        self.logger.info(f"Optimizer setup complete:")
        self.logger.info(f"  LR: {self.config['training']['learning_rate']}")
        self.logger.info(f"  Weight decay: {self.config['training']['weight_decay']}")
    
    def train_epoch(self, epoch):
        """1ã‚¨ãƒãƒƒã‚¯å­¦ç¿’"""
        self.detector.model.train()
        
        total_loss = 0.0
        total_recon_loss = 0.0
        total_anomaly_loss = 0.0
        num_batches = 0
        
        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}')
        
        for batch in pbar:
            self.optimizer.zero_grad()
            
            # å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—
            losses, output = self.detector.train_step(batch)
            
            # ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
            losses['total_loss'].backward()
            
            # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
            torch.nn.utils.clip_grad_norm_(
                self.detector.model.parameters(), 
                max_norm=1.0
            )
            
            self.optimizer.step()
            
            # æå¤±è“„ç©ï¼ˆå®‰å…¨ãªå€¤å–å¾—ï¼‰
            total_loss += self._safe_item(losses['total_loss'])
            total_recon_loss += self._safe_item(losses['reconstruction_loss'])
            total_anomaly_loss += self._safe_item(losses['anomaly_loss'])
            num_batches += 1
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼æ›´æ–°
            pbar.set_postfix({
                'Total': f"{self._safe_item(losses['total_loss']):.4f}",
                'Recon': f"{self._safe_item(losses['reconstruction_loss']):.4f}",
                'Anomaly': f"{self._safe_item(losses['anomaly_loss']):.4f}"
            })
        
        # ã‚¨ãƒãƒƒã‚¯å¹³å‡æå¤±
        avg_total_loss = total_loss / num_batches
        avg_recon_loss = total_recon_loss / num_batches
        avg_anomaly_loss = total_anomaly_loss / num_batches
        
        # è¨˜éŒ²
        self.train_history['total_loss'].append(avg_total_loss)
        self.train_history['reconstruction_loss'].append(avg_recon_loss)
        self.train_history['anomaly_loss'].append(avg_anomaly_loss)
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼æ›´æ–°
        self.scheduler.step()
        
        return avg_total_loss, avg_recon_loss, avg_anomaly_loss
    
    def validate(self):
        """æ¤œè¨¼"""
        self.detector.model.eval()
        
        val_loss = 0.0
        predictions = []
        ground_truth = []
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc='Validation'):
                images = batch['images'].to(self.device)
                labels = batch['anomaly_labels']
                
                # äºˆæ¸¬
                for i in range(images.shape[0]):
                    result = self.detector.predict(images[i:i+1])
                    predictions.append(result['anomaly_score'])
                    # tensorã®å ´åˆã®ã¿item()ã‚’ä½¿ç”¨
                    if isinstance(labels[i], torch.Tensor):
                        ground_truth.append(labels[i].item())
                    else:
                        ground_truth.append(labels[i])
                
                # æå¤±è¨ˆç®—
                output = self.detector.model(images)
                losses = self.detector.model.compute_loss(
                    images, output, labels.to(self.device)
                )
                val_loss += self._safe_item(losses['total_loss'])
        
        avg_val_loss = val_loss / len(self.val_loader)
        
        # ç•°å¸¸æ¤œçŸ¥æ€§èƒ½è©•ä¾¡
        predictions = np.array(predictions)
        ground_truth = np.array(ground_truth)
        
        # ROC-AUCè¨ˆç®—
        try:
            from sklearn.metrics import roc_auc_score, classification_report
            auc_score = roc_auc_score(ground_truth, predictions)
            
            # åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆï¼ˆé–¾å€¤ãƒ™ãƒ¼ã‚¹ï¼‰
            binary_predictions = predictions > self.detector.anomaly_threshold
            report = classification_report(
                ground_truth, 
                binary_predictions, 
                target_names=['Normal', 'Anomalous'],
                output_dict=True
            )
            
        except Exception as e:
            self.logger.warning(f"Metrics calculation failed: {e}")
            auc_score = 0.0
            report = {}
        
        validation_score = auc_score
        self.train_history['validation_score'].append(validation_score)
        
        return avg_val_loss, validation_score, report
    
    def save_checkpoint(self, epoch, is_best=False):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        models_dir = Path("models")
        models_dir.mkdir(exist_ok=True)
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        model_path = models_dir / f"minicpm_autoencoder_wallplugs_epoch{epoch+1}.pth"
        self.detector.save_model(model_path)
        
        if is_best:
            best_path = models_dir / "minicpm_autoencoder_wallplugs_best.pth"
            self.detector.save_model(best_path)
            self.logger.info(f"Best model saved: {best_path}")
        
        # å­¦ç¿’å±¥æ­´ä¿å­˜
        history_path = models_dir / "training_history_wallplugs.json"
        with open(history_path, 'w') as f:
            json.dump(self.train_history, f, indent=2)
        
        return model_path
    
    def plot_training_history(self):
        """å­¦ç¿’å±¥æ­´å¯è¦–åŒ–"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # æå¤±ã‚°ãƒ©ãƒ•
        epochs = range(1, len(self.train_history['total_loss']) + 1)
        
        axes[0, 0].plot(epochs, self.train_history['total_loss'], 'b-', label='Total Loss')
        axes[0, 0].set_title('Total Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        axes[0, 1].plot(epochs, self.train_history['reconstruction_loss'], 'g-', label='Reconstruction Loss')
        axes[0, 1].set_title('Reconstruction Loss')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Loss')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        axes[1, 0].plot(epochs, self.train_history['anomaly_loss'], 'r-', label='Anomaly Loss')
        axes[1, 0].set_title('Anomaly Detection Loss')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Loss')
        axes[1, 0].legend()
        axes[1, 0].grid(True)
        
        if self.train_history['validation_score']:
            axes[1, 1].plot(epochs, self.train_history['validation_score'], 'm-', label='Validation AUC')
            axes[1, 1].set_title('Validation AUC Score')
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('AUC Score')
            axes[1, 1].legend()
            axes[1, 1].grid(True)
        
        plt.tight_layout()
        
        # ä¿å­˜
        plot_path = Path("models") / "training_history_wallplugs.png"
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"Training history plot saved: {plot_path}")
    
    def train(self):
        """å­¦ç¿’å®Ÿè¡Œ"""
        self.logger.info("ğŸš€ MiniCPM Anomaly Detection Training Started")
        self.logger.info("="*60)
        
        best_val_score = 0.0
        patience_counter = 0
        patience = self.config['training'].get('patience', 10)
        
        start_time = datetime.now()
        
        for epoch in range(self.config['training']['epochs']):
            self.logger.info(f"\nEpoch {epoch+1}/{self.config['training']['epochs']}")
            self.logger.info("-" * 30)
            
            # å­¦ç¿’
            train_total_loss, train_recon_loss, train_anomaly_loss = self.train_epoch(epoch)
            
            # æ¤œè¨¼
            val_loss, val_score, val_report = self.validate()
            
            # ãƒ­ã‚°å‡ºåŠ›
            self.logger.info(f"Train - Total: {train_total_loss:.4f}, Recon: {train_recon_loss:.4f}, Anomaly: {train_anomaly_loss:.4f}")
            self.logger.info(f"Val - Loss: {val_loss:.4f}, AUC: {val_score:.4f}")
            
            # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«åˆ¤å®š
            is_best = val_score > best_val_score
            if is_best:
                best_val_score = val_score
                patience_counter = 0
                self.logger.info(f"ğŸ‰ New best model! AUC: {val_score:.4f}")
            else:
                patience_counter += 1
            
            # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
            self.save_checkpoint(epoch, is_best)
            
            # æ—©æœŸåœæ­¢åˆ¤å®š
            if patience_counter >= patience:
                self.logger.info(f"Early stopping triggered after {patience} epochs without improvement")
                break
        
        # å­¦ç¿’å®Œäº†
        end_time = datetime.now()
        training_time = (end_time - start_time).total_seconds()
        
        self.logger.info("\n" + "="*60)
        self.logger.info("ğŸ‰ Training Completed!")
        self.logger.info(f"Training time: {training_time:.1f} seconds")
        self.logger.info(f"Best validation AUC: {best_val_score:.4f}")
        
        # å­¦ç¿’å±¥æ­´å¯è¦–åŒ–
        self.plot_training_history()
        
        return best_val_score

def load_config():
    """è¨­å®šèª­ã¿è¾¼ã¿"""
    default_config = {
        'model': {
            'latent_dim': 512,
            'use_minicpm': True,
            'minicpm_weight': 0.3,
            'anomaly_threshold': 0.1
        },
        'training': {
            'batch_size': 4,  # MiniCPMã®ãŸã‚å°ã•ã‚
            'epochs': 50,
            'learning_rate': 1e-4,
            'weight_decay': 1e-5,
            'patience': 10
        }
    }
    
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°èª­ã¿è¾¼ã¿
    config_path = Path("config/config.yaml")
    if config_path.exists():
        with open(config_path, 'r', encoding='utf-8') as f:
            file_config = yaml.safe_load(f)
        
        # ãƒãƒ¼ã‚¸
        if 'minicpm_training' in file_config:
            default_config.update(file_config['minicpm_training'])
    
    return default_config

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("MVTec AD Wallplugs x MiniCPM ç•°å¸¸æ¤œçŸ¥å­¦ç¿’")
    print("="*60)
    
    # è¨­å®šèª­ã¿è¾¼ã¿
    config = load_config()
    print("Training Configuration:")
    print(json.dumps(config, indent=2))
    
    # GPUæƒ…å ±è¡¨ç¤º
    if torch.cuda.is_available():
        print(f"\nGPU: {torch.cuda.get_device_name()}")
        print(f"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    else:
        print("\nCPU mode (GPU not available)")
    
    try:
        # å­¦ç¿’å®Ÿè¡Œ
        trainer = WallplugsTrainer(config)
        best_score = trainer.train()
        
        print(f"\nTraining completed successfully!")
        print(f"   Best AUC Score: {best_score:.4f}")
        print(f"   Model saved: models/minicpm_autoencoder_wallplugs_best.pth")
        
    except Exception as e:
        print(f"\nTraining failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()