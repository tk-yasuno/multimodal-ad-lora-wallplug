"""
MVTec AD Wallplugs ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®Ÿç”¨ãƒ†ã‚¹ãƒˆ
å‰å‡¦ç†æ¸ˆã¿wallplugsãƒ‡ãƒ¼ã‚¿ã‚’MAD-FH FODDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§å®Ÿéš›ã«ãƒ†ã‚¹ãƒˆ
"""

import sys
from pathlib import Path
import json
import random
from datetime import datetime

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¨­å®š
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

def test_fodd_with_wallplugs():
    """FODDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§wallplugsãƒ‡ãƒ¼ã‚¿ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("MVTec AD Wallplugs x FODD ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ãƒ†ã‚¹ãƒˆ")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    data_dir = Path("data/processed/wallplugs")
    
    if not data_dir.exists():
        print("[ERROR] å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚preprocess_mvtec.pyã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return
    
    # ã‚µãƒ³ãƒ—ãƒ«ç”»åƒé¸æŠ
    categories = [
        ("train/normal", "æ­£å¸¸è¨“ç·´"),
        ("train/anomalous", "ç•°å¸¸è¨“ç·´"),
        ("validation/normal", "æ­£å¸¸æ¤œè¨¼"),
        ("validation/anomalous", "ç•°å¸¸æ¤œè¨¼")
    ]
    
    test_results = {
        "dataset": "wallplugs",
        "test_timestamp": datetime.now().isoformat(),
        "test_results": {}
    }
    
    try:
        # FODD PipelineåˆæœŸåŒ–
        from fodd_pipeline import FODDPipeline
        pipeline = FODDPipeline()
        print("[OK] FODD PipelineåˆæœŸåŒ–å®Œäº†")
        
        for category_path, category_name in categories:
            full_path = data_dir / category_path
            if not full_path.exists():
                continue
                
            # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒ«é¸æŠï¼ˆ2æšï¼‰
            image_files = list(full_path.glob("*.png"))
            sample_files = random.sample(image_files, min(2, len(image_files)))
            
            print(f"\nğŸ“ {category_name} ãƒ†ã‚¹ãƒˆ ({len(sample_files)}æš)")
            category_results = []
            
            for img_file in sample_files:
                try:
                    print(f"  ğŸ–¼ï¸  {img_file.name} åˆ†æä¸­...")
                    
                    # FODDåˆ†æå®Ÿè¡Œ
                    result = pipeline.process_single_image(str(img_file))
                    
                    # çµæœè¡¨ç¤º
                    anomaly_info = result.get("anomaly_detection", {})
                    is_anomaly = anomaly_info.get("is_anomaly", False)
                    anomaly_score = anomaly_info.get("score", 0.0)
                    confidence = anomaly_info.get("confidence", 0.0)
                    
                    similar_count = len(result.get("similar_cases", []))
                    description = result.get("generated_description", "")
                    processing_time = result.get("processing_time", 0.0)
                    
                    print(f"     ç•°å¸¸åˆ¤å®š: {'ğŸ”´ ç•°å¸¸' if is_anomaly else 'ğŸŸ¢ æ­£å¸¸'}")
                    print(f"     ç•°å¸¸ã‚¹ã‚³ã‚¢: {anomaly_score:.3f}")
                    print(f"     ä¿¡é ¼åº¦: {confidence:.3f}")
                    print(f"     é¡ä¼¼äº‹ä¾‹: {similar_count}ä»¶")
                    print(f"     å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
                    if description:
                        print(f"     AIèª¬æ˜: {description[:100]}...")
                    
                    # çµæœä¿å­˜
                    category_results.append({
                        "image_file": img_file.name,
                        "anomaly_detection": anomaly_info,
                        "similar_cases_count": similar_count,
                        "description_length": len(description),
                        "processing_time": processing_time,
                        "success": True
                    })
                    
                except Exception as e:
                    print(f"     [ERROR] ã‚¨ãƒ©ãƒ¼: {e}")
                    category_results.append({
                        "image_file": img_file.name,
                        "error": str(e),
                        "success": False
                    })
            
            test_results["test_results"][category_name] = category_results
        
        # ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼
        print("\n" + "="*60)
        print("ğŸ¯ ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
        print("="*60)
        
        total_tests = 0
        successful_tests = 0
        total_anomaly_detected = 0
        total_normal_detected = 0
        
        for category_name, results in test_results["test_results"].items():
            success_count = sum(1 for r in results if r.get("success", False))
            total_count = len(results)
            
            anomaly_count = sum(1 for r in results 
                              if r.get("success") and r.get("anomaly_detection", {}).get("is_anomaly", False))
            
            print(f"{category_name}:")
            print(f"  ãƒ†ã‚¹ãƒˆæ•°: {total_count}æš")
            print(f"  æˆåŠŸ: {success_count}æš")
            print(f"  ç•°å¸¸æ¤œçŸ¥: {anomaly_count}æš")
            
            total_tests += total_count
            successful_tests += success_count
            
            if "ç•°å¸¸" in category_name:
                total_anomaly_detected += anomaly_count
            else:
                total_normal_detected += (success_count - anomaly_count)
        
        success_rate = successful_tests / total_tests * 100 if total_tests > 0 else 0
        print(f"\n[STATS] å…¨ä½“çµ±è¨ˆ:")
        print(f"  ç·ãƒ†ã‚¹ãƒˆæ•°: {total_tests}æš")
        print(f"  æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"  ç•°å¸¸ãƒ‡ãƒ¼ã‚¿ã§ã®ç•°å¸¸æ¤œçŸ¥: {total_anomaly_detected}ä»¶")
        print(f"  æ­£å¸¸ãƒ‡ãƒ¼ã‚¿ã§ã®æ­£å¸¸åˆ¤å®š: {total_normal_detected}ä»¶")
        
        # çµæœä¿å­˜
        results_file = data_dir / "fodd_test_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(test_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ è©³ç´°çµæœä¿å­˜: {results_file}")
        print("="*60)
        
        # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ææ¡ˆ
        print("\n[NEXT] æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("1. ç•°å¸¸æ¤œçŸ¥æ€§èƒ½ã®è©³ç´°è©•ä¾¡")
        print("2. ãƒ¢ãƒ‡ãƒ«ã®è¿½åŠ å­¦ç¿’ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰")
        print("3. ä»–ã®MVTec ADãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®ãƒ†ã‚¹ãƒˆ")
        print("4. è£½å“ç’°å¢ƒã§ã®é‹ç”¨ãƒ†ã‚¹ãƒˆ")
        
    except Exception as e:
        print(f"[ERROR] ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

def create_sample_analysis():
    """ã‚µãƒ³ãƒ—ãƒ«åˆ†æç”¨ã®ä¾¿åˆ©é–¢æ•°"""
    print("\n[SAMPLE] å€‹åˆ¥ã‚µãƒ³ãƒ—ãƒ«åˆ†æä¾‹")
    
    # ç‰¹å®šã®ç”»åƒã‚’æŒ‡å®šã—ã¦åˆ†æ
    data_dir = Path("data/processed/wallplugs")
    sample_categories = [
        "train/normal",
        "train/anomalous"
    ]
    
    for category in sample_categories:
        category_path = data_dir / category
        if category_path.exists():
            image_files = list(category_path.glob("*.png"))[:1]  # 1æšã ã‘
            
            for img_file in image_files:
                print(f"\n[IMAGE] åˆ†æå¯¾è±¡: {category}/{img_file.name}")
                print(f"   ãƒ‘ã‚¹: {img_file}")
                print("   ã‚³ãƒãƒ³ãƒ‰ä¾‹:")
                print(f"   python -c \"from fodd_pipeline import FODDPipeline; p=FODDPipeline(); print(p.process_single_image('{img_file}'))\"")

if __name__ == "__main__":
    # ãƒ¡ã‚¤ãƒ³ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test_fodd_with_wallplugs()
    
    # å€‹åˆ¥åˆ†æä¾‹è¡¨ç¤º
    create_sample_analysis()