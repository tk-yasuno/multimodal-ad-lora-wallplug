"""
MVTec AD Wallplugs Ã— MiniCPM + LoRA è»½é‡å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™ç’°å¢ƒã§ã®å­¦ç¿’å®Ÿè¡Œ
"""

import sys
import torch
from pathlib import Path
import json

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¨­å®š
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def check_system_resources():
    """ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç¢ºèª"""
    print("ğŸ” ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç¢ºèª")
    print("="*40)
    
    # GPUç¢ºèª
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name()
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"âœ… GPU: {gpu_name}")
        print(f"   ãƒ¡ãƒ¢ãƒª: {gpu_memory:.1f}GB")
        
        # VRAMä½¿ç”¨é‡ç¢ºèª
        torch.cuda.empty_cache()
        allocated = torch.cuda.memory_allocated() / 1e9
        reserved = torch.cuda.memory_reserved() / 1e9
        print(f"   ä½¿ç”¨ä¸­: {allocated:.2f}GB")
        print(f"   äºˆç´„æ¸ˆã¿: {reserved:.2f}GB")
        
        return True, gpu_memory
    else:
        print("âš ï¸  GPU not available, using CPU")
        return False, 0
    
def quick_model_test():
    """è»½é‡ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ§ª è»½é‡ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ")
    print("="*40)
    
    try:
        # MiniCPMåŸºæœ¬ãƒ†ã‚¹ãƒˆ
        from src.models.minicpm_autoencoder import MiniCPMVisionEncoder
        print("âœ… MiniCPM ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
        
        # è»½é‡è¨­å®šã§ã®ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ
        test_config = {
            'input_channels': 3,
            'latent_dim': 256,  # è»½é‡åŒ–
            'input_size': (512, 512),  # ã‚µã‚¤ã‚ºå‰Šæ¸›
            'use_minicpm': False,  # åˆå›ã¯MiniCPMã‚’ç„¡åŠ¹åŒ–
            'anomaly_threshold': 0.1
        }
        
        from src.models.minicpm_autoencoder import MiniCPMAnomalyDetector
        detector = MiniCPMAnomalyDetector(test_config)
        print("âœ… è»½é‡ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False

def run_lightweight_training():
    """è»½é‡å­¦ç¿’å®Ÿè¡Œ"""
    print("\nğŸš€ è»½é‡å­¦ç¿’é–‹å§‹")
    print("="*40)
    
    # ãƒªã‚½ãƒ¼ã‚¹ç¢ºèª
    has_gpu, gpu_memory = check_system_resources()
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ
    if not quick_model_test():
        print("âŒ ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        return False
    
    # å­¦ç¿’è¨­å®šèª¿æ•´
    if gpu_memory < 8.0:
        print("âš ï¸  GPU ãƒ¡ãƒ¢ãƒªä¸è¶³ã®ãŸã‚è»½é‡è¨­å®šã‚’ä½¿ç”¨")
        batch_size = 1
        latent_dim = 256
        use_minicpm = False  # ãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚ç„¡åŠ¹åŒ–
        epochs = 5
    else:
        print("âœ… é€šå¸¸è¨­å®šã§å­¦ç¿’å®Ÿè¡Œ")
        batch_size = 2
        latent_dim = 512
        use_minicpm = True
        epochs = 10
    
    print(f"\nğŸ“‹ å­¦ç¿’è¨­å®š:")
    print(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
    print(f"   æ½œåœ¨æ¬¡å…ƒ: {latent_dim}")
    print(f"   MiniCPMä½¿ç”¨: {use_minicpm}")
    print(f"   ã‚¨ãƒãƒƒã‚¯æ•°: {epochs}")
    
    # è»½é‡å­¦ç¿’è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    config = {
        'minicpm_anomaly': {
            'model': {
                'latent_dim': latent_dim,
                'use_minicpm': use_minicpm,
                'minicpm_weight': 0.3,
                'anomaly_threshold': 0.1
            },
            'training': {
                'batch_size': batch_size,
                'epochs': epochs,
                'learning_rate': 1e-4,
                'weight_decay': 1e-5,
                'patience': 5
            }
        },
        'lora_explanation': {
            'model': {
                'name': 'Salesforce/blip2-opt-2.7b'
            },
            'lora': {
                'r': 8,  # è»½é‡åŒ–
                'alpha': 16,
                'dropout': 0.1,
                'target_modules': ['q_proj', 'v_proj']
            },
            'training': {
                'epochs': 5,  # è»½é‡åŒ–
                'batch_size': 1,
                'learning_rate': 5e-5,
                'weight_decay': 0.01,
                'warmup_steps': 20
            }
        }
    }
    
    # è¨­å®šä¿å­˜
    config_path = Path("lightweight_config.json")
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2)
    
    print(f"ğŸ’¾ è»½é‡è¨­å®šä¿å­˜: {config_path}")
    
    return True

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸš€ MVTec AD Wallplugs è»½é‡å­¦ç¿’æº–å‚™")
    print("="*50)
    
    # ãƒ‡ãƒ¼ã‚¿ç¢ºèª
    data_dir = Path("data/processed/wallplugs")
    if not data_dir.exists():
        print("âŒ å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("   preprocess_mvtec.py ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„")
        return False
    
    # ãƒªã‚½ãƒ¼ã‚¹ãƒ»ãƒ¢ãƒ‡ãƒ«ç¢ºèª
    if not run_lightweight_training():
        return False
    
    print("\nâœ… è»½é‡å­¦ç¿’æº–å‚™å®Œäº†ï¼")
    print("\nğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("1. ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«å˜ä½“ãƒ†ã‚¹ãƒˆ:")
    print("   python train_minicpm_wallplugs.py")
    print("2. LoRAãƒ¢ãƒ‡ãƒ«å˜ä½“ãƒ†ã‚¹ãƒˆ:")  
    print("   python train_lora_wallplugs.py")
    print("3. çµ±åˆå­¦ç¿’å®Ÿè¡Œ:")
    print("   python train_wallplugs_integrated.py")
    
    return True

if __name__ == "__main__":
    main()