"""
MVTec AD Wallplugs ç°¡å˜LoRAãƒ‡ãƒ¢å­¦ç¿’
æœ€å°é™ã®æ©Ÿèƒ½ã§LoRAå­¦ç¿’ã‚’å®Ÿè¡Œã™ã‚‹ãƒ‡ãƒ¢ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import sys
from pathlib import Path
import torch
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import json
from datetime import datetime
from tqdm import tqdm

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¨­å®š
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

class SimpleWallplugsDataset(Dataset):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªWallplugsãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    
    def __init__(self, data_dir, split='train', max_samples=20):
        self.data_dir = Path(data_dir)
        self.split = split
        self.max_samples = max_samples
        
        # ã‚µãƒ³ãƒ—ãƒ«åé›†
        self.samples = []
        self.collect_samples()
        
        print(f"{split} dataset: {len(self.samples)} samples")
    
    def collect_samples(self):
        """ã‚µãƒ³ãƒ—ãƒ«åé›†ï¼ˆåˆ¶é™ä»˜ãï¼‰"""
        # æ­£å¸¸ç”»åƒ
        normal_dir = self.data_dir / self.split / "normal"
        if normal_dir.exists():
            normal_files = list(normal_dir.glob("*.png"))[:self.max_samples//2]
            for img_path in normal_files:
                self.samples.append({
                    'image_path': str(img_path),
                    'label': 'normal',
                    'explanation': 'This wallplug appears to be in normal condition with proper alignment.'
                })
        
        # ç•°å¸¸ç”»åƒ
        anomalous_dir = self.data_dir / self.split / "anomalous"
        if anomalous_dir.exists():
            anomalous_files = list(anomalous_dir.glob("*.png"))[:self.max_samples//2]
            for img_path in anomalous_files:
                explanation = self.get_anomaly_explanation(img_path.name)
                self.samples.append({
                    'image_path': str(img_path),
                    'label': 'anomalous', 
                    'explanation': explanation
                })
    
    def get_anomaly_explanation(self, filename):
        """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ç•°å¸¸èª¬æ˜ç”Ÿæˆ"""
        if "overexposed" in filename:
            return "This wallplug shows overexposure issues affecting quality assessment."
        elif "underexposed" in filename:
            return "The wallplug appears underexposed with reduced surface detail visibility."
        elif "shift" in filename:
            return "This wallplug exhibits positional shift or misalignment anomaly."
        else:
            return "This wallplug shows anomalous characteristics deviating from normal standards."
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # ç”»åƒèª­ã¿è¾¼ã¿ï¼ˆã‚µã‚¤ã‚ºçµ±ä¸€ï¼‰
        image = Image.open(sample['image_path']).convert('RGB')
        image = image.resize((224, 224))  # å°ã•ã‚ã‚µã‚¤ã‚º
        
        return {
            'image': image,
            'explanation': sample['explanation'],
            'label': sample['label'],
            'path': sample['image_path']
        }

class SimpleBLIPLoRADemo:
    """ã‚·ãƒ³ãƒ—ãƒ«BLIP LoRAãƒ‡ãƒ¢"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"Device: {self.device}")
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.output_dir = Path("models/simple_lora_demo")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒ—ãƒ­ã‚»ãƒƒã‚µ
        self.processor = None
        self.model = None
        
        # å­¦ç¿’å±¥æ­´
        self.training_log = []
    
    def setup_model(self):
        """ãƒ¢ãƒ‡ãƒ«è¨­å®š"""
        try:
            from transformers import BlipProcessor, BlipForConditionalGeneration
            
            model_name = "Salesforce/blip-image-captioning-base"
            print(f"Loading model: {model_name}")
            
            # ãƒ—ãƒ­ã‚»ãƒƒã‚µèª­ã¿è¾¼ã¿
            self.processor = BlipProcessor.from_pretrained(model_name)
            print("âœ… Processor loaded")
            
            # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆè»½é‡ç‰ˆï¼‰
            self.model = BlipForConditionalGeneration.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
            ).to(self.device)
            print("âœ… Model loaded")
            
            # ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
            total_params = sum(p.numel() for p in self.model.parameters())
            print(f"Total parameters: {total_params:,}")
            
            return True
            
        except Exception as e:
            print(f"âŒ Model setup failed: {e}")
            return False
    
    def setup_data(self):
        """ãƒ‡ãƒ¼ã‚¿æº–å‚™"""
        data_dir = Path("data/processed/wallplugs")
        
        if not data_dir.exists():
            print("âŒ å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆï¼ˆå°è¦æ¨¡ï¼‰
        self.train_dataset = SimpleWallplugsDataset(data_dir, split='train', max_samples=10)
        self.val_dataset = SimpleWallplugsDataset(data_dir, split='validation', max_samples=6)
        
        return True
    
    def generate_sample(self, sample):
        """ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ"""
        try:
            image = sample['image']
            prompt = "Describe this wallplug quality:"
            
            # å…¥åŠ›æº–å‚™
            inputs = self.processor(image, prompt, return_tensors="pt").to(self.device)
            
            # ç”Ÿæˆ
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_length=50,
                    num_beams=2,
                    early_stopping=True,
                    do_sample=False
                )
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            generated_text = self.processor.decode(outputs[0], skip_special_tokens=True)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé™¤å»
            if prompt in generated_text:
                result = generated_text.replace(prompt, "").strip()
            else:
                result = generated_text
            
            return result
            
        except Exception as e:
            return f"Generation error: {str(e)}"
    
    def run_demo_training(self):
        """ãƒ‡ãƒ¢å­¦ç¿’å®Ÿè¡Œ"""
        print("\nğŸš€ Simple LoRA Demo Training")
        print("="*50)
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒ‡ãƒ¼ã‚¿æº–å‚™
        if not self.setup_model():
            return False
        
        if not self.setup_data():
            return False
        
        # å­¦ç¿’å‰ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ
        print("\nğŸ“ Pre-training samples:")
        pre_samples = []
        for i, sample in enumerate(self.train_dataset):
            if i >= 3:  # 3ã‚µãƒ³ãƒ—ãƒ«ã¾ã§
                break
            
            generated = self.generate_sample(sample)
            
            print(f"Sample {i+1}: {Path(sample['path']).name}")
            print(f"  Ground truth: {sample['explanation'][:60]}...")
            print(f"  Generated: {generated[:60]}...")
            print(f"  Label: {sample['label']}")
            print()
            
            pre_samples.append({
                'filename': Path(sample['path']).name,
                'ground_truth': sample['explanation'],
                'generated': generated,
                'label': sample['label']
            })
        
        # ãƒ‡ãƒ¢å­¦ç¿’ãƒ­ã‚°
        demo_results = {
            'timestamp': datetime.now().isoformat(),
            'model_name': "Salesforce/blip-image-captioning-base",
            'dataset': 'MVTec AD Wallplugs',
            'train_samples': len(self.train_dataset),
            'val_samples': len(self.val_dataset),
            'pre_training_samples': pre_samples,
            'notes': [
                'This is a demo run without actual LoRA training',
                'Shows baseline BLIP performance on wallplugs data',
                'Ready for LoRA fine-tuning implementation'
            ]
        }
        
        # çµæœä¿å­˜
        results_path = self.output_dir / "demo_results.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(demo_results, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Demo results saved: {results_path}")
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print("\nğŸ“Š Demo Summary:")
        print(f"   Train samples: {len(self.train_dataset)}")
        print(f"   Val samples: {len(self.val_dataset)}")
        print(f"   Model: Salesforce/blip-image-captioning-base")
        print(f"   Status: Baseline performance confirmed")
        
        print("\nğŸ’¡ Next steps:")
        print("   1. Implement LoRA configuration")
        print("   2. Add training loop with loss calculation")
        print("   3. Compare pre/post LoRA performance")
        
        return True

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸš€ MVTec AD Wallplugs Simple LoRA Demo")
    print("="*50)
    
    try:
        demo = SimpleBLIPLoRADemo()
        success = demo.run_demo_training()
        
        if success:
            print("\nâœ… Demo completed successfully!")
        else:
            print("\nâŒ Demo failed")
            
    except Exception as e:
        print(f"\nâŒ Demo error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()