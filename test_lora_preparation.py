"""
LoRAå­¦ç¿’ç°¡å˜ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
åŸºæœ¬çš„ãªLoRAå­¦ç¿’æ©Ÿèƒ½ã®å‹•ä½œç¢ºèª
"""

import sys
from pathlib import Path
import torch
from PIL import Image
import numpy as np

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¨­å®š
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def test_lora_basic():
    """LoRAåŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª LoRAåŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ")
    print("="*40)
    
    try:
        # BLIPãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã®åŸºæœ¬ãƒ†ã‚¹ãƒˆ
        from transformers import BlipProcessor, BlipForConditionalGeneration
        
        model_name = "Salesforce/blip2-opt-2.7b"
        print(f"ğŸ“± ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ: {model_name}")
        
        # ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã®ã¿èª­ã¿è¾¼ã¿
        processor = BlipProcessor.from_pretrained(model_name)
        print("âœ… ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼èª­ã¿è¾¼ã¿æˆåŠŸ")
        
        # ãƒ†ã‚¹ãƒˆç”»åƒæº–å‚™
        data_dir = Path("data/processed/wallplugs/train/normal")
        if data_dir.exists():
            test_image_path = next(data_dir.glob("*.png"))
            test_image = Image.open(test_image_path).convert('RGB')
            print(f"âœ… ãƒ†ã‚¹ãƒˆç”»åƒèª­ã¿è¾¼ã¿: {test_image_path.name}")
            
            # ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ãƒ†ã‚¹ãƒˆ
            test_text = "Describe this wallplug:"
            inputs = processor(images=test_image, text=test_text, return_tensors="pt")
            print("âœ… ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼å‡¦ç†æˆåŠŸ")
            print(f"   ç”»åƒãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶: {inputs['pixel_values'].shape}")
            print(f"   ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³æ•°: {inputs['input_ids'].shape[1]}")
            
        else:
            print("âš ï¸ ãƒ†ã‚¹ãƒˆç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        return True
        
    except Exception as e:
        print(f"âŒ LoRAåŸºæœ¬ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_lora_dataset():
    """LoRAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ“Š LoRAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ†ã‚¹ãƒˆ")
    print("="*40)
    
    try:
        from transformers import BlipProcessor
        
        # ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼æº–å‚™
        processor = BlipProcessor.from_pretrained("Salesforce/blip2-opt-2.7b")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹æº–å‚™ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        data_dir = Path("data/processed/wallplugs")
        
        # ã‚µãƒ³ãƒ—ãƒ«å–å¾—
        train_normal_images = list((data_dir / "train" / "normal").glob("*.png"))[:3]
        train_anomal_images = list((data_dir / "train" / "anomalous").glob("*.png"))[:3]
        
        print(f"âœ… æ­£å¸¸ç”»åƒã‚µãƒ³ãƒ—ãƒ«: {len(train_normal_images)}æš")
        print(f"âœ… ç•°å¸¸ç”»åƒã‚µãƒ³ãƒ—ãƒ«: {len(train_anomal_images)}æš")
        
        # ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ†ã‚¹ãƒˆ
        for i, img_path in enumerate(train_normal_images):
            image = Image.open(img_path).convert('RGB')
            text = "This wallplug appears to be in normal condition."
            
            encoding = processor(
                images=image,
                text=text,
                padding="max_length",
                truncation=True,
                max_length=64,
                return_tensors="pt"
            )
            
            print(f"   ã‚µãƒ³ãƒ—ãƒ«{i+1}: {img_path.name} -> {encoding['pixel_values'].shape}")
        
        print("âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‡¦ç†ãƒ†ã‚¹ãƒˆæˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_peft_integration():
    """PEFTçµ±åˆãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ”§ PEFTçµ±åˆãƒ†ã‚¹ãƒˆ")
    print("="*40)
    
    try:
        from peft import LoraConfig, TaskType
        
        # LoRAè¨­å®šãƒ†ã‚¹ãƒˆ
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=8,
            lora_alpha=16,
            lora_dropout=0.1,
            target_modules=['q_proj', 'v_proj'],
            bias="none"
        )
        
        print("âœ… LoRAè¨­å®šä½œæˆæˆåŠŸ")
        print(f"   Rank: {lora_config.r}")
        print(f"   Alpha: {lora_config.lora_alpha}")
        print(f"   Target modules: {lora_config.target_modules}")
        
        return True
        
    except Exception as e:
        print(f"âŒ PEFTçµ±åˆãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸš€ LoRAå­¦ç¿’äº‹å‰ãƒ†ã‚¹ãƒˆ")
    print("="*50)
    
    # GPUç¢ºèª
    if torch.cuda.is_available():
        print(f"âœ… GPU: {torch.cuda.get_device_name()}")
        print(f"   ãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    else:
        print("âš ï¸ GPU not available")
    
    # å„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    tests_passed = 0
    
    if test_lora_basic():
        tests_passed += 1
    
    if test_lora_dataset():
        tests_passed += 1
    
    if test_peft_integration():
        tests_passed += 1
    
    # çµæœè¡¨ç¤º
    print(f"\nğŸ“Š ãƒ†ã‚¹ãƒˆçµæœ: {tests_passed}/3 passed")
    
    if tests_passed == 3:
        print("âœ… ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒæˆåŠŸï¼LoRAå­¦ç¿’æº–å‚™å®Œäº†")
        print("\nğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("   1. train_lora_wallplugs.py ã§å®Ÿéš›ã®å­¦ç¿’å®Ÿè¡Œ")
        print("   2. ã‚¨ãƒãƒƒã‚¯æ•°ãƒ»ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ç’°å¢ƒã«å¿œã˜ã¦èª¿æ•´")
        return True
    else:
        print("âŒ ä¸€éƒ¨ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸ")
        return False

if __name__ == "__main__":
    main()